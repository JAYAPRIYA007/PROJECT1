#https://www.cvedetails.com
#https://www.cvedetails.com/vulnerability-list/year-1999/month-1/January.html
library(tm)
#r1=read.xlsx("health10_3.xlsx",sheetName="Sheet1")
r1=read.csv("./CorrectedCVEDataset/DataSet2015.csv")
r2=read.csv("./CorrectedCVEDataset/DataSet2016.csv")
r3=read.csv("./CorrectedCVEDataset/DataSet2017.csv")
r4=read.csv("./CorrectedCVEDataset/DataSet2018.csv")
r5=read.csv("./CorrectedCVEDataset/DataSet2019.csv")
#r1
df1<-data.frame(r1)
df2<-data.frame(r2)
df3<-data.frame(r3)
df4<-data.frame(r4)
df5<-data.frame(r5)
df<-rbind(df1,df2)
df<-rbind(df,df3)
df<-rbind(df,df4)
df<-rbind(df,df5)


catarray<-unlist(unique(df['Vulnerability.Type.s.']))
catarray<-as.vector(catarray)
catlength<-length(catarray)

#mydatframes<-list()
mydatframes<-vector("list", catlength)
myTDMat<-vector("list", catlength)
myMatrices<-vector("list", catlength)
myMat<-vector("list", catlength)
categorywiselists<-vector("list", catlength)
totalrecords<-nrow(df)
catcount<-c()
DataSet<-data.frame(Feature=character(0),Category=character(0))
AllWords<-c()
leng<-5 #'Five documents are taken from folder '
for(i3 in 1:catlength)
{
  mydatframes[[i3]]<- subset(df,Vulnerability.Type.s.==catarray[i3])
  mydatframes[[i3]]$Vulnerability.Type.s.
  rows<-nrow(mydatframes[[i3]])
  catcount<-c(c(catcount),rows)
  tmp1<- paste(mydatframes[[i3]]$Description , collapse=' ')
  
  tmp1 <- Corpus(VectorSource(tmp1))
  tmp1 <- tm_map(tmp1, removePunctuation)
  tmp1 <- tm_map(tmp1, function(x)removeWords(x,stopwords()))
  tmp1 <- tm_map(tmp1, removeWords, c("RT", "are","that"))
  myTDMat[[i3]]<-TermDocumentMatrix(  tmp1, control=list(removePunctuation=TRUE,stopwords=TRUE,removeNumbers=TRUE))#,weighting=weightBin()))
  myMatrices[[i3]]<-as.matrix(myTDMat[[i3]])
  myMat[[i3]] <- sort(rowSums(myMatrices[[i3]]),decreasing=TRUE)
  #class(myMat[[i3]])
  myMat[[i3]] <-data.frame(word=names(myMat[[i3]]),freq=myMat[[i3]])
  myMat[[i3]]['Category']<-''
  myMat[[i3]]['Category']<-catarray[i3]
 #singlecategorycontents<- paste  
  AllWords<-c(c(AllWords), myTDMat[[i3]]$dimnames$Terms)
  if(i3==1)
  {
    Mat <- myMatrices[[i3]]
  }else
  {
  Mat<-rbind(Mat, myMatrices[[i3]])
  }
}

#myTDMat[1]
myMat#[[i3]]


AllWordsCount<-length(AllWords)
AllDocumentsCount<-1

TFMat<-matrix(nrow=AllWordsCount,ncol= AllDocumentsCount)
IDFVector<-replicate(AllWordsCount,0)
TFIDF<-replicate(AllWordsCount,0)
for(i in 1:AllWordsCount)
{
  for(j in 1:AllDocumentsCount)
  {
    
    nume<- Mat[i,j]
    deno <-colSums(Mat)
    val1<-round(nume/deno ,4)[1]
    TFMat[i,j]<-val1
  }
}
TFIDFMatrix<-matrix(nrow =AllWordsCount,ncol=2)

TFIDFMatrix[1:10,]

for(i in 1:AllWordsCount)
{
  
  vect<-Mat[i,]
  val2<-length(vect[vect>0])
  IDFVector[i]<- log10(round( leng /val2,4)[1])
  
  TFIDF[i]<-  rowSums(TFMat)[i] * IDFVector[i]
  TFIDFMatrix[i,1]<-AllWords[i]
  TFIDFMatrix[i,2]<-TFIDF[i]
} 

#Line 7
k<-0
AllWordsSorted<-AllWords
AllWordsCountMinusOne<-AllWordsCount-1
for(i in 1:AllWordsCountMinusOne)
{
  k<-i+1
  for(j in k:AllWordsCount)
  {
    if(TFIDF[i] <TFIDF[j])
    {
      tmp1<-  TFIDF[i]
      TFIDF[i]<-TFIDF[j]
      TFIDF[j]<-tmp1
      
      tmp1<- AllWordsSorted[i]
      AllWordsSorted[i]<-AllWordsSorted[j]
      AllWordsSorted[j]<-tmp1
    }
  }
}
tfidf_df_sorted <-data.frame(word=AllWordsSorted,freq=TFIDF)
#Line 8 and 9
if( nrow(tfidf_df_sorted )  >200)
  tfidf_df_sorted<-tfidf_df_sorted[1:200,]
tfidf_df_sorted
v1<-as.vector(tfidf_df_sorted$word)
v1[1]
v1leng<-length(v1)
str1<-''
for(i in 1:v1leng)
{
  str1 <-paste(str1,trimws(as.character( v1[i])),'\n' ,collapse='',sep='')
}
#write_lines(x=str1,path="TFIDFfileoutput.txt")
v2<-as.vector(tfidf_df_sorted$freq)
v2leng<-length(v2)
str2<-''
for(i in 1:v1leng)
{
  str2 <-paste(str2,trimws(as.character( v2[i])),'\n' ,collapse='',sep='')
}
#library(readr)
#write_lines(x=str2,path="TFIDFValuefileoutput.txt")

#write.csv(as.vector(tfidf_df_sorted$freq),file="TFIDFValuefileoutput.txt")
#TEXT PROCESSING ENDS HERE============================





#===================================================================================
#empirical entropy H (D) of data set D is calculated as follows.  (4)
AllWords<-  as.vector(tfidf_df_sorted$word)
AllWordsCount<-length(AllWords)

FeaturesinCategories<- vector("list",catlength) #3 for hw,sw and swt
DataSet<-data.frame(Feature=character(0),Category=character(0))
for(i in 1:AllWordsCount)
{
  for(j in 1:catlength)
  {
    if (AllWords[i] %in% myMat[[j]]$word)
    { 
      FeaturesinCategories[[j]]<- c(c(FeaturesinCategories[[j]]),AllWords[i])
      DataSet <- rbind(DataSet,  data.frame(Feature=AllWords[i],Category=catarray[j]))
      
    }
  }
  # if (AllWords[i] %in% catarray[1])
  # {
  #   FeaturesinCategories[[1]]<- c(c(FeaturesinCategories[[1]]),AllWords[i])
  #   DataSet <- rbind(DataSet,  data.frame(Feature=AllWords[i],Category='Hardware'))
  # }
  # if (AllWords[i] %in% swlist)
  # {
  #   FeaturesinCategories[[2]]<- c(c(FeaturesinCategories[[2]]),AllWords[i])
  #   DataSet <- rbind(DataSet,  data.frame(Feature=AllWords[i],Category='Software'))
  # }
  # if (AllWords[i] %in% swtlist)
  # {
  #   FeaturesinCategories[[3]]<- c(c(FeaturesinCategories[[3]]),AllWords[i])
  #   DataSet <- rbind(DataSet,  data.frame(Feature=AllWords[i],Category='Testing'))
  # }
}
#=========================================
DSSize<-nrow(DataSet) #AllWordsCount
AllWords<-as.vector(DataSet$Feature)
AllWordsCount<-DSSize
FeaturesCount<-c()
for(i in 1:catlength)
{
  FeaturesCount<-c(c(FeaturesCount), length( FeaturesinCategories[[i]]) )
}
#Features1Count<-length( FeaturesinCategories[1])
op1<-0
tmpsum<-0
for(i in 1:catlength)
{
  if(FeaturesCount[i]/DSSize>0)
  {
  tmpsum <-tmpsum + ((FeaturesCount[i]/DSSize)*log2(FeaturesCount[i]/DSSize) )
  }
}
op1 <- -tmpsum
#op1 <- -  ( ((Features1Count/DSSize)*log2(Features1Count/DSSize) )             +((Features2Count/DSSize)*log2( Features2Count/DSSize) )            +((Features3Count/DSSize)*log2( Features3Count/DSSize) )  )
EntrophyHofD <- (op1)

cat('Entropy H(D):\n')
print(EntrophyHofD)


#-((3/11) *log2(3/11) )  - (5/11 *log2(5/11)) -  ((3/11) *log2(3/11) ) 
#- ( (14/30)  * log2(14/30) ) -((8/30)  * log2(8/30) )-((8/30)  * log2(8/30) )
#-(( (1/4)  * log2(1/4) ) + ((2/4)  * log2(2/4) ) +((1/4)  * log2(1/4) ))
#-(( (3/11)  * log2(3/11) ) + ((5/11)  * log2(5/11) ) +((3/11) * log2(3/11) ))


overallsum<-0
#n<-DSSize
KCate<-catlength
HDofA<-c()


InformationGainForAllWords<-c()

for(i in 1:DSSize)
{
  #Calculation of Child Entropy 
  #Refer HelpMust-InfoGain.pdf in this folder 7th page
  Word<-AllWords[i]
  deno1<- floor(DSSize /2)
  deno2<-  DSSize   - deno1
  child1<-DataSet[ 1:deno1 ,]
  child2<-DataSet[ deno1+1:deno2,]
  #--------------
  Child1Size<-nrow(child1) 
  child1Subsets<-vector("list",catlength)
  child1SubsetsCount<-c()
  for(j in 1:catlength)
  {
  child1Subsets[[j]] <-subset ( child1,  Category==catarray[j] & Feature==Word)
  child1SubsetsCount<-c(c(child1SubsetsCount), nrow(child1Subsets[[j]]))
  }
  
  #child1HwSubset <-  subset ( child1,  Category=='Hardware' & Feature==Word)
  #child1SwSubset <-  subset ( child1,  Category=='Software' & Feature==Word)
  #child1SwtSubset <-  subset ( child1,  Category=='Testing' & Feature==Word)
  #child1Hwcount<-nrow(child1HwSubset)
  #child1Swcount<-nrow(child1SwSubset)
  #child1Swtcount<-nrow(child1SwtSubset)
  #child1Hwcount
  #child1Swcount
  #child1Swtcount
  
  op1<-0
  tmpsum<-0
  for(k in 1:catlength)
  {
    op1<-0
    if( is.infinite( log2(  child1SubsetsCount[k]/Child1Size   ) ))
    {
      
    }else
    {
      op1<-( child1SubsetsCount[k]/Child1Size ) * log2(   child1SubsetsCount[k]/Child1Size   ) 
    }
    tmpsum<- tmpsum + op1
  }
  child1Entropy <- - tmpsum

  
  Child2Size<-nrow(child1) 
  child2Subsets<-vector("list",catlength)
  child2SubsetsCount<-c()
  for(j in 1:catlength)
  {
    child2Subsets[[j]] <-subset ( child2,  Category==catarray[j] & Feature==Word)
    child2SubsetsCount<-c(c(child2SubsetsCount), nrow(child2Subsets[[j]]))
  }
  
  #child1HwSubset <-  subset ( child1,  Category=='Hardware' & Feature==Word)
  #child1SwSubset <-  subset ( child1,  Category=='Software' & Feature==Word)
  #child1SwtSubset <-  subset ( child1,  Category=='Testing' & Feature==Word)
  #child1Hwcount<-nrow(child1HwSubset)
  #child1Swcount<-nrow(child1SwSubset)
  #child1Swtcount<-nrow(child1SwtSubset)
  #child1Hwcount
  #child1Swcount
  #child1Swtcount
  
  op1<-0
  tmpsum<-0
  for(k in 1:catlength)
  {
    op1<-0
    if( is.infinite( log2(  child2SubsetsCount[k]/Child2Size   ) ))
    {
      
    }else
    {
      op1<-( child2SubsetsCount[k]/Child2Size ) * log2(   child2SubsetsCount[k]/Child2Size   ) 
    }
    tmpsum<- tmpsum + op1
  }
  child2Entropy <- - tmpsum
  
  
  WeightedAverageEntropyofChildren <-  ( (Child1Size/ DSSize) * child1Entropy  ) +
    ( (Child2Size/ DSSize) * child2Entropy  )
  
  InformationGain <- EntrophyHofD - WeightedAverageEntropyofChildren
  
  InformationGainForAllWords <- c(c(InformationGainForAllWords), InformationGain)
}




OutputDF<-data.frame(Feature=character(0),GainValue=numeric(0))
UniqueWords<-c()
UniqueWordsValue<-c()
for(i in 1:DSSize)
{
  if(i==1)
  {
    UniqueWords<-c(c(UniqueWords),AllWords[i])
    val1<- EntrophyHofD -  InformationGainForAllWords [i]
    UniqueWordsValue<-c(c(UniqueWordsValue), val1)
    
    OutputDF<- rbind( OutputDF,data.frame(Feature= AllWords[i] , GainValue=val1 )  )
  }
  if( AllWords[i] %in% UniqueWords )
  {
    
  }else
  {
    UniqueWords<-c(c(UniqueWords),AllWords[i])
    val1<- EntrophyHofD -  InformationGainForAllWords [i]
    UniqueWordsValue<-c(c(UniqueWordsValue), val1)
    OutputDF<-    rbind( OutputDF,data.frame(Feature= AllWords[i] , GainValue =val1 )  )
  }
}
OutputDF


k<-0
UniqueWordsCount<-length(UniqueWords)
UniqueWordsMinusOne<-UniqueWordsCount-1
for(i in 1:UniqueWordsMinusOne)
{
  k<-i+1
  for(j in k:UniqueWordsCount)
  {
    if(UniqueWordsValue[i] < UniqueWordsValue[j])
    {
      tmp1<-  UniqueWordsValue[i]
      UniqueWordsValue[i]<-UniqueWordsValue[j]
      UniqueWordsValue[j]<-tmp1
      
      tmp1<- UniqueWords[i]
      UniqueWords[i]<-UniqueWords[j]
      UniqueWords[j]<-tmp1
    }
  }
}
OutputDF <-data.frame(word=UniqueWords,freq=UniqueWordsValue)
OutputDF

#
# 

# 
# 
# totalcount<- subset(totcount, select =c(1,  ncol(totcount)))
# barplotvalues<-   (totalcount)
# barplotvalues<-data.frame(t(barplotvalues))
# barplotvalues<-as.matrix(barplotvalues)
XAxisTitle <-UniqueWords
barplotvalues<-UniqueWordsValue
barplot(barplotvalues,names.arg=XAxisTitle,col=c('red','blue'),beside=TRUE ,main="FEATURE WORDS WITH INFORMATION GAIN VALUE",xlab="FEATURE WORD",ylab="GAIN VALUE")


XAxisTitle <-UniqueWords[1:10]
barplotvalues<-UniqueWordsValue[1:10]
barplot(barplotvalues,names.arg=XAxisTitle,col=c('red','blue'),beside=TRUE ,main="FEATURE WORDS WITH INFORMATION GAIN VALUE",xlab="FEATURE WORD",ylab="GAIN VALUE")

# legend("topleft", c('Program','BugReport'),fill=c('red','blue'))
# 


#C. THE WORD VECTOR SPACE ESTABLISHMENT PAGE 6 OF 8 IN BASE PAPER
#Intheapplicationofthispaper,eachvulnerabilitydescription is expressed as an m-dimensional vector 
#(m is the number of feature words in the feature word set) 
m<-UniqueWordsCount
str1<-''

for(j in 1:m)
{
   df[600,'Description']
}


#df$Vulnerability.Type.s.
#UniqueWordsCount

df['VectorEncoded']<-''
dfrows<-nrow(df)
for(i in 1:dfrows)
{
  vs<- df[i,4] #'Description']  
  cnninputvector<-''
for(j in 1:UniqueWordsCount )
{
  
  if( grepl (UniqueWords[j] ,vs))
  {
    cnninputvector <- paste(cnninputvector,'1',sep='')
  }else
  {
    cnninputvector <- paste(cnninputvector,'0',sep='')
  }
}
  df[i,'VectorEncoded']<-cnninputvector
}
df$VectorEncoded
#Sort records based on Vulnerability.Type.s. Category field
df<-df[order(df$Vulnerability.Type.s.),]



#df$Vulnerability.Type.s.
#colnames(df)
#length(df$VectorEncoded[1])
#Project Ends Here
#=============================================================
# 
# if(FALSE)
# {
# #GradDescent
# #https://www.kaggle.com/bryanb2003/gradient-descent-machine-learning-in-r
# GradD <- function(x, y, alpha = 0.006, epsilon = 10^-10){
#   iter <- 0
#   i <- 0
#   x <- cbind(rep(1,nrow(x)), x)
#   theta <- matrix(c(1,1),ncol(x),1)
#   cost <- (1/(2*nrow(x))) * t(x %*% theta - y) %*% (x %*% theta - y)
#   delta <- 1
#   while(delta > epsilon){
#     i <- i + 1
#     theta <- theta - (alpha / nrow(x)) * (t(x) %*% (x %*% theta - y))
#     cval <- (1/(2*nrow(x))) * t(x %*% theta - y) %*% (x %*% theta - y)
#     cost <- append(cost, cval)
#     delta <- abs(cost[i+1] - cost[i])
#     if((cost[i+1] - cost[i]) > 0){
#       print("The cost is increasing.  Try reducing alpha.")
#       return()
#     }
#     iter <- append(iter, i)
#   }
#   print(sprintf("Completed in %i iterations.", i))
#   return(theta)
# }
# 
# 
# data("iris")
# library(gradDescent)
# x <- as.matrix(iris[,c(2:4)])
# y <- as.matrix(iris[,1])
# stheta <- GradD(scale(x), y, alpha = 0.006, epsilon = 10^-10)
# stheta
# 
# data("cars")
# x <- as.matrix(cars$speed)
# y <- as.matrix(cars$dist)
# theta <- GradD(x, y, alpha = 0.006, epsilon = 10^-10)
# theta
# }